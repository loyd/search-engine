\section{Ранжирование}
Запрос~--- набор слов с учётом порядка.

Ранжирование~--- упорядочивание документов по принципу наибольшего соответствия запросу.

Фактор (признак)~--- характеристика свойств документа, запроса или их взаимоотношение.

Виды факторов:
\begin{enumerate}
  \item Статические факторы~--- зависящие только от документа;
  \item Факторы, зависящие только от запроса;
  \item Динамические факторы~--- зависящие и от документа, и от запроса.
\end{enumerate}

Функция ранжирования~--- алгоритм, выполняющий ранжирование, используя множество факторов. Такая функция может быть комбинацией других функций ранжирования, используя, например, средневзвешенную сумму или обученную модель регрессии \cite{segaran07}.

Так же факторы условно можно разделить на следующие классы:
\begin{enumerate}
  \item По содержимому: наличие слов из запроса, их частоты, положения;
  \item Ссылочные: популярность страницы, цитируемость;
  \item Метрики, основанные на ответной реакции пользователей.
\end{enumerate}

В данной работе внимание уделено первым двум классам факторов. Класс ответной реакции, функции ранжирования которого часто машинно-обучены, не рассматриваются, так как для получения пригодных результатов требуется достаточно длительное живое обучение, которые в рамках работы не достижимо.


\subsection{Ранжирование по содержимому}
Данный класс основывается на информации, которую можно получить непосредственно из документа:
\begin{itemize}
  \item Общее количество слов в документе (длина документа);
  \item Частота вхождения запрошенных слов в документ;
  \item Положение запрошенных слов в документе;
  \item Расстояние между запрошенными словами в документе;
  \item Входят ли запрошенные слова в заголовки.
\end{itemize}


\subsubsection{TF-IDF} \label{sssec:tf-idf}
Простейшим динамическим фактором является количество вхождения слова $q$ запроса $Q$ ($q\in Q$) в документ $d$: $n_q^d$.

Однако использование данного фактора приводит к тому, что более длинные страницы оказываются более релевантными, даже если относительная доля искомого слова невелика. Поэтому имеет смысл говорить о частоте слова.

Частота слова (от англ. TF~--- term frequency)~--- другой простейший динамический фактор, определяемый как
\begin{equation} \label{eq:tf}
  tf(q, d)=\frac{n_q^d}{|d|}.
\end{equation}

Тогда функция ранжирования на основе только этого фактора примет вид:
\begin{equation}
  score^{tf}(d, Q) = \sum_{q\in Q} tf(q, d).
\end{equation}

Данный фактор позволяет ранжировать страницы исходя из того, сколько раз на ней встретилось искомое слово. Так, выполняя поиск по слову <<python>>, пользователь ожидает увидеть документы, где это слово встречается часто, а не документ о музыканте, который где-то в конце упомянул, что у него дома живёт питон.

Однако данный фактор обладает существенным недостатком: в случае, если в запрос входят популярные (часто встречаемые) слова, то большая часть веса будет приходится именно на них. Для решения этой проблемы вводят обратную частоту документа.

Обратная частота документа (от англ. IDF~--- inverse document frequency)~--- инверсия частоты, с которой слово встречается в документах. Учёт IDF уменьшает вес широкоупотребительных слов.
\begin{equation} \label{eq:idf}
  idf(q)=\log\frac{N}{N_q},
\end{equation}
\begin{conditions}
  $N$ & количество документов в коллекции, $N=|D|$;\\
  $N_q$ & количество документов, содержащих слово $q$, $N_q=|\{d: d\ni q\}|$.
\end{conditions}

Объединяя (\ref{eq:tf}) и (\ref{eq:idf}):
\begin{equation}
  score^{tfidf}(d, Q) = \sum_{q\in Q} tf(q, d) \cdot idf(q, D).
\end{equation}

Теперь вес некоторого слова пропорционален количеству употребления этого слова в документе, и обратно пропорционален частоте употребления слова в других документах коллекции.


\subsubsection{BM25} \label{sssec:bm25}
BM25~--- TF-IDF-подобная функция ранжирования, имеющая лучшую вероятностную интерпретацию \cite{robertson09}:
\begin{equation} \label{eq:bm25}
  score^{bm25}(d, Q) = \sum_{q\in Q}idf(q)\cdot\frac{n_q^d\cdot (k+1)}{n_q^d + k\cdot\left(1-b+b\cdot\frac{|d|}{avg(|d|)}\right)}
\end{equation}
\begin{conditions}
  $k$ и $b$ & свободные коэффициенты, $b\in[0..1]$ и $k \geq 0$;\\
  $avg(|d|)$ & средняя длина документов в коллекции.
\end{conditions}

В данном методе часто используют <<сглаженные>> варианты $idf$, например:
\begin{equation}
  idf(q)=\log\frac{N - N_q + 0.5}{N_q + 0.5}.
\end{equation}

Вышеуказанная формула IDF имеет следующую особенность. Для слов, входящих в более чем половину документов из коллекции, значение IDF отрицательно. Таким образом, при наличии любых двух почти идентичных документов, в одном из которых есть слово, а в другом~--- нет, второй может получить большую оценку. Простейшим решением является игнорирование отрицательных слагаемых в сумме, что эквивалентно игнорированию соответствующих высокочастотных слов:
\begin{equation}
  idf(q)=\max\left(\log\frac{N - N_q + 0.5}{N_q + 0.5}, 0\right).
\end{equation}

Существуют модификации метода, позволяющие уменьшить влияние длины документа для очень больших документов: BM25l \cite{lv11+} и BM25+ \cite{lv11l}, однако в данной работе эта проблема решается восстановлением выбросов (раздел \ref{sssec:outlier}).

Для объединения значений BM25 по разным полям документа (текст, заголовки, входящие ссылки и др.) часто используют различные вариации метода BM25F \cite{robertson09}. Однако в данной работе BM25 является лишь одной из многих частей итоговой функции ранжирования и проблема объединения решена нормализацией каждой части в отдельности (раздел \ref{sssec:normalization}).


\subsubsection{Положение в документе} \label{sssec:position}
Обычно, если страница релевантна поисковому слову, то это слово расположено близко к началу страницы, быть может, даже находится в заголовке. Чтобы воспользоваться этим наблюдением, поисковая система может приписывать результату больший ранг, если поисковое слово встречается в начале документа:
\begin{equation}
  score^{pos}(d, Q)=\sum_{q\in Q}p_q^d,
\end{equation}
где $p_q^d$~--- порядковый номер первого вхождения слова $q$ в документ $d$.


\subsubsection{Расстояние между словами}
Если запрос содержит несколько слов, то часто бывает полезно ранжировать результаты в зависимости от того, насколько близко друг к другу встречаются поисковые слова. Как правило, вводя запрос из нескольких слов, человек хочет найти документы, в которых эти слова концептуально связаны \cite{segaran07}.

\begin{equation}
  score^{dist}(d, Q)=\sum_{\substack{i<j \\ q_i, q_j\in Q}} |p_{q_j}^d - p_{q_i}^d|.
\end{equation}

Эту же функцию ранжирования можно использовать при поиске точного совпадения (фраза, заключённая в кавычки), указав для неё большой вес.

Однако данная функция ранжирования обладает существенным недостатком: это единственная функция из рассмотренных, которая требует хранения в индексе всех вхождений слов в документ, что многократно увеличивает индекс и, как следствие, время запроса. Поэтому в данной работе эта функция не используется.


\subsection{Ссылочное ранжирование}
Все обсуждавшиеся до сих пор функции ранжирования были основаны на содержимом документа. Хотя многие поисковые системы до сих пор работают таким образом, часто результаты можно улучшить, приняв во внимание, что сказано об этом документе в других. Это особенно полезно при индексировании документов сомнительного содержания или таких, которые могли быть созданы спамерами, поскольку маловероятно, что на такие документы ссылаются релевантные.


\subsubsection{Простой подсчёт ссылок}
Простейший способ работы с внешними ссылками заключается в простом подсчёте их количества. Так обычно оцениваются научные работы: считается, что их значимость тем выше, чем чаще их цитируют.

\begin{equation}
  score^{inb}(d) = \big|\{(d_i, d)\in L\}\big|,
\end{equation}
где $L$~--- мн-во всех ссылок $(d_i, d_j)$.

\subsubsection{PageRank} \label{sssec:pagerank}
Этот алгоритм приписывает каждому документу ранг, оценивающий его значимость. Значимость документа вычисляется исходя из значимости ссылающихся на него документов и общего количества ссылок, имеющихся на каждом из них:
\begin{equation} \label{eq:pagerank}
  pr(d) = 0.15 + 0.85 \cdot \left(\sum_{(d_i, d)\in L} \frac{pr(d_i)}{|L_{d_i}|}\right),
\end{equation}
где $L_d$~--- мн-во всех ссылок с документа $d$: $L_d = \{(d, d_j)\in L\}$.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{scope}[every node/.style={circle,thick,draw}]
      \node (A) at (-1.5,0) {$A (0.8)$};
      \node (B) at (1,3.5) {$B (0.42)$};
      \node (C) at (2.5,1) {$C (?)$};
      \node (E) at (2.5,-3.5) {$E (0.6)$};
      \node (F) at (6,3) {$F (0.9)$};
    \end{scope}

    \begin{scope}[>={Stealth[black]},
      every node/.style={fill=white,circle},
      every edge/.style={draw=red,very thick}]
      \path[->] (A) edge node {$0.2$} (C);
      \path[->] (E) edge node {$0.3$} (C);
      \path[->] (F) edge node {$0.3$} (C);
      \path[->] (A) edge (B);
      \path[->] (A) edge (E);
      \path[->] (B) edge (F);
      \path[->] (E) edge (F);
      \path[->] (F) edge[bend left=40] (E);
      \path[<-] (E) edge ++(-2,0);
      \path[->] (A) edge ++(0,-2);
      \path[<-] (A) edge ++(-2.3,0);
      \path[->] (F) edge ++(2,0);
      \path[<-] (F) edge ++(0,2);
      \path[<-] (B) edge ++(-3,0);
    \end{scope}
  \end{tikzpicture}
  \caption{Вычисление ранга PageRank документа $C$.}
  \label{fig:pagerank}
\end{figure}

Например, PageRank страницы $C$ (рис. \ref{fig:pagerank}) вычисляется как
\begin{equation}
  \begin{split}
    pr(C) &= 0.15 + 0.85\cdot\left(\frac{pr(A)}{|L_A|}+\frac{pr(F)}{|L_F|}+\frac{pr(E)}{|L_E|}\right)\\
    &= 0.15 + 0.85\cdot\left(\frac{0.8}{4}+\frac{0.9}{3}+\frac{0.6}{2}\right)\\
    &= 0.15 + 0.85\cdot 0.8 = 0.83.
  \end{split}
\end{equation}

Увы, имеется небольшая ловушка~--- в данном примере для всех документов, ссылающихся на $C$, уже вычислен ранг, что является тривиальным случаем. Необходимо вычислить ранги для множества документов, ранги которых ещё не известны.

Решение состоит в том, чтобы присвоить всем документам произвольный начальный ранг (отличный от нуля, например 1) и провести несколько итераций. Количество необходимых итераций зависит от числа страниц и в нашем случае (количество страниц до миллиона) 20-30 должно быть достаточно. Google, например, просчитывает ранги для своего индекса приблизительно за 100 итераций.

Функция ранжирования в данном случае предельно проста:
\begin{equation}
  score^{pr}(d) = pr(d).
\end{equation}


\subsubsection{Использование текста ссылки}
Ещё один полезный способ ранжирования результатов~--- использование текста ссылок на документ при определении степени её релевантности запросу. Часто удаётся получить более качественную информацию из того, что сказано в ссылках, ведущих на документ, чем из самого документа.

Таким образом, учитывается ранг источников тех ссылок, которые ведут на оцениваемую страницу и содержат слова из запроса:
\begin{equation}
  score^{link}(d, Q) = \sum_{\substack{(d_i, d, t)\in L \\ q\in Q\cap t}} pr(d_i),
\end{equation}
где $t$ в $(d_i, d, t)$~--- текст ссылки $(d_i, d)$.


\subsection{Объединение функций ранжирования}
Теперь, имея все необходимые функции ранжирования, необходимо получить итоговую функцию, которая будет являться средневзвешенной суммой нормализованных функций ранжирования.


\subsubsection{Нормализация} \label{sssec:normalization}
Чтобы сравнивать результаты, получаемые различными функциями ранжирования, необходимо как-то нормализовать их, то есть привести к одному и тому же диапазону и направлению: от $0$ (наихудший результат) до $1$ (наилучший результат):
\begin{equation} \label{eq:simple-norm}
  norm(s, s_{min}, s_{max}) = \begin{cases}
    \frac{s-s_{min}}{s_{max} - s_{min}}, \; s_{max} > s_{min}\\
    1, \; \text{иначе}
  \end{cases},
\end{equation}
где $s$~--- результат функции ранжирования.

В случае, когда $s_{min}=s_{max}$, то есть значение функции для всех документов одинаково, будем считать, что все документы получили максимальную оценку.


\subsubsection{Обработка выбросов} \label{sssec:outlier}
Выброс~--- результат измерения, выделяющийся из общей выборки. Например, среди документов, полученных по запросу <<ecmascript>>, встречается спецификация, а так как ссылок, включающих искомое слово, на неё ведёт много, то и $score^{link}$ для данного документа будет сильно выше других документов. Это приведёт к дискредитации $score^{link}$~--- остальные документы будут иметь низкие показатели. Таким образом, необходимо ввести определённый верхний (нижний) порог, выше (ниже) которого функция обращается в $1$ ($0$).

Простейший метод определения выброса основан на межквартильном расстоянии: выбросами считается всё, что не попадает в диапазон
\begin{equation}
  \left[(s_{25}-1.5\cdot (s_{75}-s_{25})), (s_{75}+1.5\cdot (s_{75}-s_{25}))\right],
\end{equation}
\begin{conditions}
  $s_{25}$ & 0.25-квантиль;\\
  $s_{75}$ & 0.75-квантиль.
\end{conditions}

Однако использование такого критерия приводит к тому, что по некоторым функциям ранжирования $0$ или $1$ не достижимы (максимум или минимум внутри диапазона). Поэтому имеет смысл в качестве диапазона брать $[s_{min}', s_{max}']$, где
\begin{equation}
  \begin{split}
    s_{min}'&=\max(s_{min}, s_{25}-1.5\cdot (s_{75}-s_{25})),\\
    s_{max}'&=\min(s_{max}, s_{75}+1.5\cdot (s_{75}-s_{25})).
  \end{split}
\end{equation}


\subsubsection{Итоговая функция ранжирования}
Теперь, если функцию нормировки (\ref{eq:simple-norm}) переписать как
\begin{equation}
  norm(s) = \begin{cases}
    1, \; (s > s_{max}')\lor (s_{max}' = s_{min}')\\
    0, \; s < s_{min}'\\
    \frac{s-s_{min}'}{s_{max}' - s_{min}'}, \text{иначе}
  \end{cases},
\end{equation}
то итоговую формулу для функции ранжирования можно задать как средневзвешенную сумму всех нормализованных функций:
\begin{equation}
  score(d, Q)=\frac{\sum w_s\cdot norm(score^s(d, Q))}{\sum w_s}.
\end{equation}
